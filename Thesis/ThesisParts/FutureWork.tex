\newpage   
\chapter{Future Work}

Currently, the cost function for distribution of sub-tasks among of the slave nodes is calculated using the size of the bounding box to fit the print object i.e. the volume of the bounding box for the object. This results in balanced distributed of the load such that none of the slave nodes are over-worked. Although, load balancing is one of the criteria for distribution, there are can be other criteria used as well. One possible criteria would be using the surface area of the print object as a measure for cost calculation. Complex surfaces are difficult to express with less number of triangles in the mesh representation making the beginning steps of the pipeline costly i.e. the process of converting the mesh to voxels. The more the surface voxels, higher is the cost of half-toning as cost of half-toning is proportional to the surface voxels. The complexity of the texture with high resolution leads to higher cost of rasterization process. The computation time can be expressed as a function \textit{f(s,o)} where \textit{s} is the ratio of total number of surface voxels to total number of voxels and \textit{o} is the ratio of total number of occupied voxels to total number of voxels. The ratios can be calculated using the Equation \ref{eq:s} and Equation \ref{eq:o} where \begin{math}Num\_{o} \end{math} is the number of occupied voxels, \begin{math} TNum\_{v} \end{math} is total number of voxels, \begin{math}Num\_{s} \end{math} is the number of surface voxels. Determining the correct form of the function f opens up new avenue for future work. One possible approach for determining the form of the function f, could be a data-drive approach wherein the information related to computation time for each print object, ratio of surface voxels to total number of voxels (s) and ratio of occupied voxel to total number of voxels (o) can be recorded on each slave node. Logging this information and then use of statistical analysis to find the correct form the function could be done. Using this function, the computation time for each print object can be predicted which could be used to implement a new cost function used for distributing the objects between the compute nodes.  

\begin{equation}
\label{eq:s}
\begin{aligned}
s= Num\_{s} / TNum\_{v}
\end{aligned}
\end{equation}   

\begin{equation}
\label{eq:o}
\begin{aligned}
o=  Num\_{o} / TNum\_{v}
\end{aligned}
\end{equation}   

The placement of the print objects on the print bed also strongly influences the appearance properties. This is because of the variance in time for which the slices are exposed to the UV light in the curing process which in turn affects the appearance properties. Companies in animation industry, for example Laika, use 3D printing to print the animation characters with the series of expressions. It is important that the 3D prints of the closest (in the series) facial expressions have the same appearance properties. If these print objects are placed some distance apart, worst being in the extreme corners of the print bed, then it would lead to the print having noteworthy difference in appearance. So locality of the print objects could be one of the other possibilities which could be used for distribution of the print objects among the slave nodes. \newline 

Another possibility for future work is changing the underlying communication protocol used i.e. replace MPI with TCP/IP or REST protocol. As seen in the section \ref{ProtoComp}, the receive function makes the major impact  on the performance of Prototype II, it would be interesting to know if the performance improves on using another communication protocol. Logging functionality can be used to understand the behavior of the distributed cuttlefish by generating the information of the run times of different nodes for different models with varying textures. This logged information along with the node configuration i.e. system features like the CPU speed, cores, memory etc can be used to make informed decisions about work load allocation if the cluster nodes remain unchanged. \newline

Changes to the underlying distributed system architecture could impact the performance of the distributed cuttlefish application, for example as all the computers in the enterprise use the common network, there can be contention among nodes. Generally within the cluster, the compute nodes and the master nodes are connected via dedicated private network and only the master node is connected to the public network. As the cluster nodes which communicate with the master node have their own private network, there could be lower contention which might increase the performance for Prototype II as it is heavily impacted by the Receive function. For Prototype I, the master node can become the bottleneck for performing deserialization for large number of slave nodes. If there is possibility of using a faster hard drive on the master node, may be the cost of deserialization can be reduced for large number of slave nodes. Replacing cluster computing with cloud computing would be big leap, where in the distributed cuttlefish is offered as a service on the cloud. \newline   
