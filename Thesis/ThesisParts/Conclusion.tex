\chapter{Conclusion}

With research fields like 3d Printing reaching new horizons, it is possible to print high-fidelity print objects due to the features offered by the new generation 3D Printers for high-resolution multi-material prints. One of the known drawbacks of 3D Printing is that it is slow and mass production of small objects takes too much time making it is costly, as the speed at which the printing can happen is strongly influenced by the material used for printing. Printing these objects economically to look as close as possible to the real-world objects requires a lot processing to be done on the digital input given to the software. Currently Cuttlefish can print multiple objects at the same time reducing the over-all production time for the prints. Although, cuttlefish is capable of performing the computation to enable multiple prints at the same time, it is limited by the machine capacity i.e. the resources offered by the single machine on which the application runs. \newline

To remove system resources as the limitation to perform large scale computation, distributed version of the cuttlefish printer driver was developed through the thesis to allow the use of available resources by running the application on the cluster of machines. Two prototypes of the distributed cuttlefish version were implemented using the classic master-slave parallel processing pattern. The major difference between the two implemented prototypes was the form in which the input was given to the slaves and partial output communicated by the slaves to the master. In Prototype I, the communication of the workload from the master node to the slave nodes was done by writing a configuration file per slave node and communicating the file path to the slave node. In Prototype II, the communication of the workload from the master to slaves was in form of stream of bytes consisting of serialized print job per slave node. The partial output communicated by the slaves to the master was by writing the serialized partial slices to the disk in Prototype I whereas in Prototype II the serialized slices were compressed and then sent as stream of bytes to the master. The goal of implementing Prototype II was to enable distributed cuttlefish to execute even without a networked file system.\newline 

The distributed system consisted of cluster of heterogeneous processors connected via enterprise network with access to a networked file system. MPI was used to implement the communication between the cluster nodes. Manual testing of the two prototypes was done to check if the application ran as expected for discussed scenarios and automated testing of the two prototypes was done to measure the performance of the application for varying workload and cluster size. Both distributed cuttlefish prototype I and prototype II performed better than non-distributed cuttlefish for varying number and types of models. The speed-up gained by both the prototypes was at-least linear and in some cases super-linear. One of the possible reasons for super-linear speed up is the cache hit rate being higher for smaller data size in multi-core processors. When the models are distributed among the processors, the total amount of data per processor is reduced in comparison to single processor performing computation on all the models at the same time, leading to smaller size of data to be cached. This behavior, i.e smaller data size decreases the computation time due to more cache hits, is also seen when cuttlefish is executed multiple times with each run having single model rather than one time with more number of models. Another probable reason for high speed-up in distributed cuttlefish is that even though the RAM is limited, it is sufficient to keep all the necessary data in RAM unlike in non-distributed cuttlefish where higher number of models lead to data being swapped in/out of the RAM leading to disk read/writes which are costly. This justifies use of distributed computing as a solution to perform large scale computation. \newline

Prototype II performed better than Prototype I in the various scenarios tested and the percentage increase in efficiency of Prototype II in comparison to the Prototype I scaled with the increase in cluster size. The Prototype II was implemented with two designs which lead to varying performance of the application. In Design I, the print objects transformation set by the master is used by the slaves to generate the partial slices whereas in Design II, the print objects are re-arranged at the slave leading to much smaller partial slices. The comparison of the two designs revealed that the performance of prototype II is highly affected by the amount of data to be communicated by the slaves as well as the size of the partial slices, i.e. bigger slices lead to higher computation by certain components of the pipeline. \newline

The performance of prototypes for varying size of the models was tested with constant number of models and increasing the model size and the observation was that the speed-up gained by the prototypes decreases with increase in models size. This behavior could be justified as increase in model size increases the workload. The performance of distributed cuttlefish Prototype I is highly impacted by the serialization of the slices as it requires disk I/O whereas in Prototype II, receiving the partial slices as stream of bytes impacts the performance strongly. For Prototype I, when the size of the models increases the time taken to deserialize the slices increases. One probable reason is that deserialization of the slices leads to allocation of large memory buffers for slices of bigger model sizes in RAM . As the system has limited RAM, the use of large amount buffers exhausts the available memory leading to frequent paging of the least used pages out of the memory (the master node used for the test-cases had 32 GiB RAM and the pagefile.sys was 24.5 GiB while running deserialization method for 5 16cm head model, showing that the processor was swapping the pages at a high rate.)  \newline
