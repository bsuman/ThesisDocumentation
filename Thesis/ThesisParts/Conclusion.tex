\newpage   
\chapter{Conclusion}

With research fields like 3d Printing reaching new horizons, it is possible to print high-fidelity print objects due to the features offered by the new generation 3D Printers for high-resolution and multi-material prints. One of the known drawbacks of 3D Printing is that it is slow and mass production of small objects takes too much time making it is costly, as the speed at which the printing can happen is strongly influenced by the material used for printing. Printing these objects economically to look as close as possible to the real-world objects requires a lot processing to be done on the digital input given to the software which generates the information that drives the 3D Printers. Currently Cuttlefish can print multiple objects at the same time reducing the over-all production time for the prints. Although, cuttlefish is capable of performing the computation to enable multiple prints at the same time, it is limited by the machine capacity i.e. the resources offered by the single machine on which the application runs. \newline

To remove system resources as the limitation to perform large scale computation, distributed version of the cuttlefish printer driver was developed through the thesis to allow use of available resources by running the application on the cluster of machines. Two prototypes of the distributed cuttlefish version were implemented using the classic master-slave parallel processing pattern. The difference between the two implemented prototype was the form in which the input was given to the slaves and partial output communication from the slaves to the master. In Prototype I, the communication of the workload from the master node to the slave nodes was done by writing a configuration file per slave node and communicating the file path to the slave node. In Prototype II, the communication of the workload from the master to slaves was in form of stream of bytes consisting of serialized print job per slave node. The partial output communicated by the slaves to the master was writing the serialized partial slices to the disk in Prototype I whereas in Prototype II the serialized slices were compressed and then sent as stream of bytes. The goal of implementing Prototype II was to enable distributed cuttlefish to execute even without a networked file system.\newline 

The distributed system consisted of cluster of heterogeneous processors connected via enterprise network with access to a networked file system. MPI was used to implement the communication between the cluster nodes. Manual testing of the two prototypes was done to check if the application ran as expected for discussed scenarios and automated testing of the two prototypes was done to measure the performance of the application for varying workload and cluster size. Both Prototype I and Prototype II performed better than Non-distributed Cuttlefish for varying number and types of models. The speed-up gained by both the prototypes was at-least linear and in some cases super-linear. One of the possible reasons for super-linear speed up is the cache hit rate being higher for smaller data size in multi-core processors. When the models are distributed among the processors, the total amount of data per processor is reduced in comparison to one processor performing computation on all the models at the same time, leading to smaller size of data to be cached. This behavior, i.e smaller data size decreases the computation time due to more cache hits, is also seen when cuttlefish is executed multiple times with each run having different models rather than one time with more number of models. Another probable reason for high speed-up in distributed cuttlefish is that even though the RAM is limited, it is sufficient to keep all the necessary data in RAM unlike in non-distributed cuttlefish where higher number of models lead to data being swapped in/out of the RAM leading to disk read/writes which are costly. This justifies use of distributed computing as a solution to perform large scale computation. \newline

Prototype II performed better than Prototype I in the various scenario tested and the percentage increase in efficiency of Prototype II in comparison to the Prototype I scaled with the increase cluster size. The Prototype II were implemented with two designs which lead varying performance of the application. In Design I, the print objects transformation set by the master is used by the slaves to generate the partial slices whereas in Design II, the print objects are re-arranged at the slave leading to much smaller partial slices. The comparison of the two designs revealed that the performance of prototype II is highly affected by the amount of data to be communicated by the slaves as well as the size of the partial slices, i.e. bigger slices lead to higher computation by certain components of the pipeline. \newline


