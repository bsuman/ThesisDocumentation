\chapter{Results and Analysis}

After the implementation phase of the design discussed in the previous chapter, the next phase involved performing tests to check if the implementation worked. To recapitulate the problem statement intended to be solved through the thesis was to increase the performance of the cuttlefish application by adding more computational capacity and distributing the objects to utilize the added computational power. The first part of the goal was achieved by creating a distributed system of heterogeneous systems as compute nodes connected through the enterprise network. For utilizing the computational capacity added, the application had to be modified to perform distribution of the work items amongst the computes nodes, collect their partial results and use the partial result to generate the final output. The desired result to be achieved through the thesis primarily was less computation time for multiple models as compared to computation time needed by current cuttlefish application and increase resource utilization by making use of the available machines.\newline

\section{Overview of SDLC Testing Phase} \label{SDLCTesting}

In the software development life-cycle, testing phase follows the implementation phase wherein various tests are conducted to verify and validate the implemented software. The result of the testing phase depends highly upon the type of test cases which are run. Testing can be classified into various categories depending on criteria chosen for classification, for example: the methods chosen to conduct tests, levels of testing etc. On the basis of methods used for testing, testing can be performed by manually testing the software or by automating the tests. In manual testing, the tester uses the software as an end-user and makes note of the deviations(if any) seen in the behavior of the system as from the expected. In automated testing, another software or scripts are used to run the various test-cases for the software under consideration. Each of these broadly classified types of testing techniques can consists of unit testing, integration testing, system testing, performance testing, regression testing etc.\newline

Manual testing as stated earlier is done by the tester by running the various use-cases in which an end user might execute the system. Mostly, manual testing is done to verify the system's behavior with respect to the specification which involves unit testing, integration testing. The validation of the implementation is done with respect to satisfying the needs of the customer which generally involves user acceptance tests and usability tests. Automated testing is generally done to perform regression testing where in part of the software under consideration is being fixed for some bug found by verification process, by re-writing a piece of code or introducing the fix by a new piece of code. The goal of regression tests is to ensure that the fixed bug is not leading to newer bugs in the system and thus is done quite frequently i.e. each time a discovered bug is fixed. Automation testing is also used to conduct stress test or performance tests where in the system is subjected to different loads and the behavior is observed with the primary goal to measure the performance or efficiency of the system under varying loads. As automation testing involves using other software or scripts to perform the tests, it cheaper and efficient both in terms of money and time. \newline 

On the basis of level of testing, testing can be done to check the functional or non-functional requirements. Functional requirements cover what the software is intended to do i.e. for a given input, the systems performs the intended task and the results generated by the system are correct. Non-functional requirements from the system involve checking the performance of the system, the resource utilization etc. \textbf{Functional testing} can broken down further into discrete levels as follows: 

\begin{itemize}
\item{\textbf{Unit Testing}}- Generally, it is performed by the developer who is aware about the code and the new modules which are implemented. The unit which is tested is generally doing a particular task / dedicated task and is a smaller portion of the whole code. The goal of the test is to ensure that each unit is performing the functionality it is implemented for. Unit tests generally have quite limited scope as it is not possible to cover all the execution paths of a code and the test data used by the developer is not the same as the test data to be used by the quality assurance team. 
\item{\textbf{Integration Testing}}- As the name suggests, integration testing is done to check if the individual modules tested via unit testing work correctly in unison. The unit testing of lower-level modules can be done first followed by integration tests by combining one or more modules which is called as bottom-up approach. If all the system modules together are tested i.e. the highest-level of module is tested first followed by testing each module individually later, it is called top-down approach.
\item{\textbf{System Testing}}- System testing is generally done by a specialized quality assurance team where in the system is tested as a whole to check if the system meets the quality standards agreed upon. The system is tested in an environment which is very similar to the production environment i.e. the environment where the developed application is going to be deployed.
\item{\textbf{Acceptance Testing}}- One of the most crucial steps of testing, the QA team verifies if the application works correctly for test scenarios which are discussed prior to the implementation by the customer and the development team. These test cases are drawn on the basis of the acceptance criteria for the software. 
\end{itemize}        

\textbf{Non-Functional testing} as mentioned earlier includes testing the non-functional requirements of the software such as the performance of the system, security, ease of use of the system etc. Non-Functional testing can be broken down into following levels: 

\begin{itemize}
\item{\textbf{Performance Testing}}- Performance testing involves exposing the system under consideration to various levels of stress and loads to monitor the behavior of the system. The goal of this type of testing is to identify the performance issues or bottlenecks which could be caused by network delay, large data input size, hitting peak processing capacity etc.  
\item{\textbf{Usability Testing}}- This type of testing involves the checking the ease of use for the software where the possible improvements in the software are noted by observing the users use the software. It involves checking the efficiency of use, learn-ability, error messages seen by the user and satisfaction of the user.
\item{\textbf{Portability Testing}}- If the software is built with multi-platform support, then this type of testing checks the functioning of the software on different platforms. The diversity in the platforms covers checking different operating systems, different underlying architecture or type of network.    
\item{\textbf{Security Testing}}- The intention of security testing is to find any vulnerabilities in the software which would compromise the security guarantees that should be provided by the software. The checks are done to ensure confidentiality, integrity, authentication, availability, authorization, non-repudiation, buffer over-flow vulnerabilities etc. 
\end{itemize} 

\section{Implementation Assessment}

Amongst the types of testing discussed in the Section \ref{SDLCTesting}, the implementation of the thesis was tested by using manual testing during the development and automated testing after the development was done. The manual testing involved running the scenarios to check individual components or parts of the components.  
Following the unit tests, were the tests which involved checking how the components worked together for different scenarios. Some of the manual tests performed are described in Section  \ref{Manual Tests}. 

\subsection{Manual Tests}

Out of the various test cases which were conducted, the important cases have been documented in this Section which highlight the scenarios testing the components newly introduced for distributed computing. 

\begin{enumerate}
\item{Test-cases specific for Prototype I}: The prototype I tests included testing the \textit{MasterDistributor}, \textit{SlaveReporter} and \textit{MasterMerger} components. 
\begin{itemize}
\item{\textbf{Test-Case for \textit{MasterDistributor} Component}}:

\begin{itemize}
\item{\textbf{Scenario}}- To test if the \textit{MasterDistributor} component generates correct distribution for a given set of input objects and cluster size. The goal is to test the correct calculation of the threshold function, creation of distributed print object list and using the generated list correctly writing the main configuration file for each slave node.  
\item{\textbf{Method}}- Distributed cuttlefish was executed multiple times with varying inputs. The test data for the simplest scenario consisted of 2 head models to be distributed among 3 node cluster with 2 slaves and one master node. Each head was 8cms and the expected balanced distribution of the input would to be generate load of one head model per slave node. The same test case was run repeatedly with varying the number of input objects, different types of input models i.e. varying the test-data and cluster size. 
\item{\textbf{Results}}- As in prototype I, the \textit{MasterDistributor} component generated main configuration file per slave and the output of the current test was verified by checking the number of print objects in the main configuration file for each slave node. Another way to check the threshold calculation was to use simple standard output statements or logging the information in a trace file.
\end{itemize}

\item{\textbf{Test-Case for \textit{SlaveReporter} Component}}: 
\begin{itemize}
\item{\textbf{Scenario}}- The \textit{SlaveReporter} Component generates the output with partial slices, which are written to the disk after serialization and chunk-wise sending the path to the master. To test if the \textit{SlaveReporter} component generates correct partial output and the path sent to the master is the correct location.
\item{\textbf{Method}}- Distributed cuttlefish was executed on the slave node. To check if the serialization of the partial slices happens correctly, the slices are deserialized at the slave and the partial output is written to the disk as images. The part of code implementing the test i.e. deserialization of the slices and generating images of the partial slices is not part of the final code which is used to build the distributed cuttlefish binary but is rather just code to test the scenario. 
\item{\textbf{Results}}- The images of the partial slices generated by the \textit{SlaveReporter} component were thoroughly checked to ensure correct material assignment. Ensuring that the partial slices are correctly serialized helped to narrow down the errors which were encountered in generation of the merged slices.  
\end{itemize}

\item{\textbf{Test-Case for \textit{MasterMerger} Component}}: 
\begin{itemize}
\item{\textbf{Scenario}}- The \textit{MasterMerger} Component generates the output with merged slices, which are written to the disk in a chunk-wise fashion after de-serialization. To test if the \textit{MasterMerger} component generates correct merged output.
\item{\textbf{Method}}- Distributed cuttlefish was executed in the cluster.To check if the de-serialization of the partial slices happens correctly at the master, the de-serialized partial slices are written to the disk as images before writing to the merged full slices. The part of code implementing the test i.e. deserialization of the slices and generating images of the partial slices is not part of the final code which is used to build the distributed cuttlefish binary but is rather just code to test the scenario. 
\item{\textbf{Results}}- The images of the partial slices generated by the \textit{MasterMerger} component were thoroughly checked to ensure correct material assignment. The final output of the \textit{MasterMerger} component i.e. the merged full slices were verified by checking the output of the \textit{SliceDebug} component output. Through this test a bug in the code was detected wherein the empty full slices created before writing the partial slices were not initialized with empty voxel value i.e. no material assigned these voxels, leading to random value assignment to them which lead to incorrect results, i.e. slices had random patches of materials assigned. 
\end{itemize}
\end{itemize}  

\item{\textbf{Test-cases specific for Prototype II}}: The testing of prototype II was done through the test cases for \textit{MasterPJDistributor}, \textit{SlavePJCollector}, \textit{SlaveReporter} and \textit{MasterMerger} 

\begin{itemize}
\item{\textbf{Test-Case for \textit{MasterPJDistributor} Component}}:

\begin{itemize}
\item{\textbf{Scenario}}- To test if the \textit{MasterPJDistributor} component generates correct distribution for a given set of input objects and cluster size. The goal is to test the correct calculation of the threshold function, creation of distributed print object list and using the generated list correctly to generate the print job per slave node.
\item{\textbf{Method}}- Distributed cuttlefish was executed multiple times with varying inputs. The test data for the simplest scenario consisted of 2 head models to be distributed among 3 node cluster with 2 slaves and one master node. Each head was 8cms and the expected balanced distribution of the input would to be generate load of one head model per slave node. The same test case was run repeatedly with varying the number of input objects, different types of input models i.e. varying the test-data and cluster size. 
\item{\textbf{Results}}- As in prototype II, the \textit{MasterPJDistributor} component generated print job per slave and the output of the current test was verified by debugging the number of print objects in the print job for each slave node. Another way to check the threshold calculation was to use simple standard output statements or logging the information in a trace file. Through this test, a bug in the code which led to incorrect threshold calculation was caught. The incorrect threshold calculation lead incorrect number of print objects in the print jobs leading to one of the computes nodes being overloaded. Another bug related to placement of the print objects in the print bed was discovered as well, where in the print jobs sent to the slaves had print objects with the transformation created by the print job organizer at the master. This lead to generation of partial slices with the print objects placed at some displacement from the origin at the slave. This bug was later fix by changing the behavior of the \textit{MasterMerger} component for prototype II.
\end{itemize}

\item{\textbf{Test-Case for \textit{SlavePJCollector} Component}}:

\begin{itemize}
\item{\textbf{Scenario}}- To test if the \textit{SlavePJCollector} component receives correct input from the master. The goal is to test if the \textit{SlavePJCollector} component correctly receives the serialized print job, deserializes the print job and loads the texture information to the GPU memory after receiving it from the master and deserializing the received texture data.
\item{\textbf{Method}}- Distributed cuttlefish was executed multiple times on the slave node with varying number of input models at the master.
\item{\textbf{Results}}- In the prototype II, the \textit{SlavePJCollector} component receives the serialized print job from the master. The size of the serialized data was cross-examined with the size of data sent at the master node. After deserializing the print job, the number of print objects contained in the print job was checked through debugging the component. To check if the texture data was received and deserialized correctly, the deserialized data was written to an image on the disk. Through this test, a bug in the format in which the texture data was uploaded to the GPU memory at the slave node was found. The texture data is in RGBA format and is linearized at the master node before uploading the data to master node GPU. When the texture data is communicated to the slave, the load operation should not perform the linearization again and instead load the data directly to the GPU memory.
\end{itemize}



\end{enumerate}

  

\section{Results}